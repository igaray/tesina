Incremental Compilation

Sept. 8, 2016 · Michael Woerister

Introduction

    I remember when, during the 1.0 anniversary presentation at the Bay Area Meetup, Aaron Turon talked about Dropbox so far having been pretty happy with using Rust in production there.
    The core team has been in touch with them regularly, he said, asking them, you know, what do you need? And their answer is always: faster compiles ...
    Improving compile times has actually been a major development focus after Rust reached 1.0 -- although, up to this point, much of the work towards this goal has gone into laying architectural foundations within the compiler and we are only slowly beginning to see actual results.
    One of the projects that is building on these foundations, and that should help improve compile times a lot for typical workflows, is incremental compilation.
    Incremental compilation avoids redoing work when you recompile a crate, which will ultimately lead to a much faster edit-compile-debug cycle.

Why Incremental Compilation in the First Place?

    Gran parte del tiempo de un programador se pasa en el ciclo de trabajo editar-compilar-debuguear:

    - se realiza un pequeño cambio (frecuentemente en un unico modulo o funcion),
    - se corre el compilador para convertir el codigo en un objeto ejecutable,
    - se ejecuta el programa resultante o un conjunto de pruebas unitarias para ver el resultado del cambio.

    Despues de eso, se vuelve al primer paso, realizar otro pequeño cambio informado por el conocimiento adquirido en la iteracion previa.
    Este bucle de alimentacion esencial es el nucleo de la actividad diaria de un programador.
    Se busca que el tiempo que se pasa detenido mientras se espera que el compilador produzca el compilador ejecutable sea lo mas breve posible.

    La compilacion incremental es una forma de aprovechar el hecho que poco cambia entre compilaciones durante el flujo de trabajo normal.
    Muchos, si no la mayoria, de los cambios entre sesiones de compilacion solo tienen un impacto local en el codigo maquina del binario producido, mientras que la mayor parte del programa, al igual que a nivel codigo, termina igual, bit a bit.
    La compilacion incremental apunta a retener la mayor parte posible de estas partes sin cambios a la vez que se rehace solo la cantidad de trabajo que debe hacerse.

How Do You Make Something "Incremental"?

    Ya se detallo que computar algo incrementalmente significa actualizar solo aquellas partes de la salida de la computacion que necesita ser adaptada en respuesta a los cambios dados en las entradas de la computacion.
    Una estrategia basica que podemos emplear para lograr esto es ver una computacion grande (tal como compilar un programa completo) como una composicion de muchas computaciones pequeñas interrelacionadas que construyen una sobre otra.
    Cada una de estas computaciones mas pequeñas producira un resultado intermedio que puede ser cacheado y reutilizado en una iteracion subsiguiente, evitando la necesidad de re-computar ese resultado intermedio en particular.

    Let's see how this scheme translates to the compiler.

An Incremental Compiler

    La forma en que se eligio implementar la incrementalidad en el compilador de Rust es directa: una sesion de compilacion incremental sigue exactamente los mismos pasos que una sesion de compilacion batch.
    Sin embargo, cuando el flujo de control llegue a un punto en el cual esta a punto de computar un resulto intermedio no trivial, intentara cargar ese resultado del cache de compilacion incremental en disco en su lugar.
    Si existe una entrada valida en el cache, el copmilador puede saltear la computacion de ese dato en particular.
    Este es un esquema (simplificado) de de las diferentes fases de compilacion y los resultados intermedios que producen:

    FIGURE Compiler Phases and their By-Products

    Primero, el compilador parse el codigo fuente en un arbol de sintaxis abstracto (AST).
    El AST pasa luego a la fase de analisis que produce informacion de tipos y el MIR para cada funcion.
    Luego de eso, si el analisis no encontro ningun error, la fase de generacion de codigo transformara la version MIR del programa en su version de codigo maquina, emitiendo un archivo objeto por cada modulo de codigo.
    En la ultima fase todos los archivo objeto son linkeados juntos en el binario final, que puede ser una libreria o un ejecutable.
    Hasta ahora las cosas parecen bastante simples: en lugar de computar algo por segunda vez, sencillamente cargar el valor desde el cache.
    Las cosas se complican cuando es necesario saber si es efectivamente valido usar un valor del cache o si hay que recomputarlo porque alguna entrada ha cambiado.

Dependency Graphs

    Existe un modelo formal que puede usarse para modelar los resultados intermedios de una computacion y su propiedad de estar actualizad de una manera directa: los grafos de dependencias.
    Cada entrada y cada resultado intermedio son representados como un nodo en un grafo dirigido.
    Los arcos en el grafo representan cual resultado intermedio o entrada puede tener influencia en otro resultado intermedio.
    En general no se puede asumir que los grafos de dependencias son arboles, sino grafos dirigidos aciclicos.
    Lo que convierte a esta estructura de datos en realmente util es que permite realizar consultas tales como "si X ha cambiado, entonces Y aun esta actualizado?".
    Para resolver esta consulta se examina un nodo Y y se colectan las entradas de las cuales Y depende transitivamente, siguiendo los arcos salientes de Y.
    Si alguna de esas entradas cambio, el valor cacheado para Y esta desactualizado y debe ser recomputado.

Dependency Tracking in the Compiler

    When compiling in incremental mode, we always build the dependency graph of the produced data: every time, some piece of data is written (like an object file), we record which other pieces of data we are accessing while doing so.

    The emphasis is on recording here. At any point in time the compiler keeps track of which piece of data it is currently working on (it does so in the background in thread-local memory). This is the currently active node of the dependency graph. Conversely, the data that needs to be read to compute the value of the active node is also tracked: it usually already resides in some kind container (e.g. a hash table) that requires invoking a lookup method to access a specific entry. We make good use of this fact by making these lookup methods transparently create edges in the dependency graph: whenever an entry is accessed, we know that it is being read and we know what it is being read for (the currently active node). This gives us both ends of the dependency edge and we can simply add it to the graph. At the end of the compilation sessions we have all our data nicely linked up, mostly automatically:

    FIGURE Dependency Graph of Compilation Data

    This dependency graph is then stored in the incremental compilation cache directory along with the cache entries it describes.

    At the beginning of a subsequent compilation session, we detect which inputs (=AST nodes) have changed by comparing them to the previous version. Given the graph and the set of changed inputs, we can easily find all cache entries that are not up-to-date anymore and just remove them from the cache:

    FIGURE Using the Dependency Graph to Validate the Incremental Compilation Cache

    Anything that has survived this cache validation phase can safely be re-used during the current compilation session.

    There are a few benefits to the automated dependency tracking approach we are employing. Since it is built into the compiler's internal APIs, it will stay up-to-date with changes to the compiler, and it is hard to accidentally forget about. And if one still forgets using it correctly (e.g. by not declaring the correct active node in some place) then the result is an overly conservative, but still "correct" dependency graph: It will negatively impact the re-use ratio but it will not lead to incorrectly re-using some outdated piece of data.

    Another aspect is that the system does not try to predict or compute what the dependency graph is going to look like, it just keeps track. A large part of our (yet to be written) regression tests, on the other hand, will give a description of what the dependency graph for a given program ought to look like. This makes sure that the actual graph and the reference graph are arrived at via different methods, reducing the risk that both the compiler and the test case agree on the same, yet wrong, value.

"Faster! Up to 15% or More."*

    Let's take a look at some of the implications of what we've learned so far:

        The dependency graph reflects the actual dependencies between parts of the source code and parts of the output binary.
        If there is some input node that is reachable from many intermediate results, e.g. a central data type that is used in almost every function, then changing the definition of that data type will mean that everything has to be compiled from scratch, there's no way around it.

    In other words, the effectiveness of incremental compilation is very sensitive to the structure of the program being compiled and the change being made. Changing a single character in the source code might very well invalidate the whole incremental compilation cache. Usually though, this kind of change is a rare case and most of the time only a small portion of the program has to be recompiled.

    The Current Status of the Implementation

    For the first spike implementation of incremental compilation, what we call the alpha version now, we chose to focus on caching object files. Why did we do that? Let's take a look at the compilation phases again and especially at how much time is spent in each one on average:

    Relative Cost of Compilation Phases

    As you can see, the Rust compiler spends most of its time in the optimization and codegen passes. Consequently, if this phase can be skipped at least for part of a code base, this is where the biggest impact on compile times can be achieved.

    With that in mind, we can also give an upper bound on how much time this initial version of incremental compilation can save: If the compiler spends X seconds optimizing when compiling your crate, then incremental compilation will reduce compile times at most by those X seconds.

    Another area that has a large influence on the actual effectiveness of the alpha version is dependency tracking granularity: It's up to us how fine-grained we make our dependency graphs, and the current implementation makes it rather coarse in places. For example, the dependency graph only knows a single node for all methods in an impl. As a consequence, the compiler will consider all methods of that impl as changed if just one of them is changed. This of course will mean that more code will be re-compiled than is strictly necessary.

Performance Numbers

    Here are some numbers of how the current implementation fares in various situations. First let's take a look at the best case scenario where a 100% of a crate's object files can be re-used. This might occur when changing one crate in a multi-crate project and downstream crates need to be rebuilt but are not really affected.

    Normalized Incremental Compilation Build Times

    As you can see, compiling a crate for the first time in incremental mode can be slower than compiling it in non-incremental mode. This is because the dependency tracking incurs some additional cost when activated. Note that compiling incrementally can also be faster (as in the case of the regex crate). This is because incremental compilation splits the code into smaller optimization units than a regular compilation session, resulting in less time optimizing, but also in less efficient runtime code.

    The last column shows the amount of time a rebuild of the crate takes when nothing has actually changed. For crates where the compiler spends a lot of time optimizing, like syntex-syntax or regex, the gain can be substantial: The incremental rebuild only takes about 22% of the time a full rebuild would need for syntex-syntax, only 16% for regex, and less than 10% for the all.rs test case of the futures-rs crate.

    On the other hand, for a crate like the futures-rs library that results in little machine code when being compiled, the current version of incremental compilation makes little difference: It's only 3% faster than compiling from scratch.

    The next graph shows which impact various changes made to the regex crate have on incremental rebuild times:

    Build Times After Specific Changes

    The numbers show that the effectiveness of incremental compilation indeed depends a lot on the type of change applied to the code. For changes with very local effect we can get close to optimal re-use (as in the case of Error::cause(), or dfa::write_vari32()). If we change something that has an impact on many places, like Compiler::new(), we might not see a noticeable reduction in compile times. But again, be aware that these measurements are from the current state of the implementation. They do not reflect the full potential of the feature.

Future Plans

    The alpha version represents a minimal end-to-end implementation of incremental compilation for the Rust compiler, so there is lots of room for improvement. The section on the current status already laid out the two major axes along which we will pursue increased efficiency:

        Cache more intermediate results, like MIR and type information, which will allow the compiler to skip more and more steps.

        Make dependency tracking more precise, so that the compiler encounters fewer false positives during cache invalidation.

    Improvements in both of these directions will make incremental compilation more effective as the implementation matures.

    In terms of correctness, we tried to err on the side of caution from the get-go, rather making the compiler recompute something if we were not sure if our dependency tracking did the right thing, but there is still more that can be done.

        We want to have many more auto-tests that make sure that various basic components of the system don't regress. This is an area where interested people can start contributing with relative ease, since one only needs to understand the Rust language and the test framework, but not the more complicated innards of the compiler's implementation. If you are interested in jumping in, head on over to the tracking issue on GitHub and leave a comment!

        We are working on the cargo incremental tool (implemented as a Cargo subcommand for hassle-free installation and usage) that will walk a projects git history, compiling successive versions of the source code and collecting data on the efficiency and correctness of incremental versus regular compilation. If you're interested in helping out, consider yourself invited to either hack on the tool itself or downloading and running it on a project of yours. The #rustc channel on IRC is currently the best place to get further information regarding this.
