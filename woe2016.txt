Incremental Compilation

Sept. 8, 2016 · Michael Woerister

Introduction


Why Incremental Compilation in the First Place?

    Gran parte del tiempo de un programador se pasa en el ciclo de trabajo editar-compilar-debuguear:

    - se realiza un pequeño cambio (frecuentemente en un unico modulo o funcion),
    - se corre el compilador para convertir el codigo en un objeto ejecutable,
    - se ejecuta el programa resultante o un conjunto de pruebas unitarias para ver el resultado del cambio.

    Despues de eso, se vuelve al primer paso, realizar otro pequeño cambio informado por el conocimiento adquirido en la iteracion previa.
    Este bucle de alimentacion esencial es el nucleo de la actividad diaria de un programador.
    Se busca que el tiempo que se pasa detenido mientras se espera que el compilador produzca el compilador ejecutable sea lo mas breve posible.

    La compilacion incremental es una forma de aprovechar el hecho que poco cambia entre compilaciones durante el flujo de trabajo normal.
    Muchos, si no la mayoria, de los cambios entre sesiones de compilacion solo tienen un impacto local en el codigo maquina del binario producido, mientras que la mayor parte del programa, al igual que a nivel codigo, termina igual, bit a bit.
    La compilacion incremental apunta a retener la mayor parte posible de estas partes sin cambios a la vez que se rehace solo la cantidad de trabajo que debe hacerse.

How Do You Make Something "Incremental"?

    Ya se detallo que computar algo incrementalmente significa actualizar solo aquellas partes de la salida de la computacion que necesita ser adaptada en respuesta a los cambios dados en las entradas de la computacion.
    Una estrategia basica que podemos emplear para lograr esto es ver una computacion grande (tal como compilar un programa completo) como una composicion de muchas computaciones pequeñas interrelacionadas que construyen una sobre otra.
    Cada una de estas computaciones mas pequeñas producira un resultado intermedio que puede ser cacheado y reutilizado en una iteracion subsiguiente, evitando la necesidad de re-computar ese resultado intermedio en particular.

    Let's see how this scheme translates to the compiler.

An Incremental Compiler

    La forma en que se eligio implementar la incrementalidad en el compilador de Rust es directa: una sesion de compilacion incremental sigue exactamente los mismos pasos que una sesion de compilacion batch.
    Sin embargo, cuando el flujo de control llegue a un punto en el cual esta a punto de computar un resulto intermedio no trivial, intentara cargar ese resultado del cache de compilacion incremental en disco en su lugar.
    Si existe una entrada valida en el cache, el copmilador puede saltear la computacion de ese dato en particular.
    Este es un esquema (simplificado) de de las diferentes fases de compilacion y los resultados intermedios que producen:

    FIGURE Compiler Phases and their By-Products

    Primero, el compilador parse el codigo fuente en un arbol de sintaxis abstracto (AST).
    El AST pasa luego a la fase de analisis que produce informacion de tipos y el MIR para cada funcion.
    Luego de eso, si el analisis no encontro ningun error, la fase de generacion de codigo transformara la version MIR del programa en su version de codigo maquina, emitiendo un archivo objeto por cada modulo de codigo.
    En la ultima fase todos los archivo objeto son linkeados juntos en el binario final, que puede ser una libreria o un ejecutable.
    Hasta ahora las cosas parecen bastante simples: en lugar de computar algo por segunda vez, sencillamente cargar el valor desde el cache.
    Las cosas se complican cuando es necesario saber si es efectivamente valido usar un valor del cache o si hay que recomputarlo porque alguna entrada ha cambiado.

Dependency Graphs

    Existe un modelo formal que puede usarse para modelar los resultados intermedios de una computacion y su propiedad de estar actualizad de una manera directa: los grafos de dependencias.
    Cada entrada y cada resultado intermedio son representados como un nodo en un grafo dirigido.
    Los arcos en el grafo representan cual resultado intermedio o entrada puede tener influencia en otro resultado intermedio.
    En general no se puede asumir que los grafos de dependencias son arboles, sino grafos dirigidos aciclicos.
    Lo que convierte a esta estructura de datos en realmente util es que permite realizar consultas tales como "si X ha cambiado, entonces Y aun esta actualizado?".
    Para resolver esta consulta se examina un nodo Y y se colectan las entradas de las cuales Y depende transitivamente, siguiendo los arcos salientes de Y.
    Si alguna de esas entradas cambio, el valor cacheado para Y esta desactualizado y debe ser recomputado.

Dependency Tracking in the Compiler

    Al compilar en modo incremental, siempre se construye el grafo de dependencia de los datos producidos: cada vez que se escribe un dato (como un archivo objeto), se registra cuales otros datoa se acceden en el proceso.
    Se hace enfasis en el registro. En todo momento el compilador mantiene el registro de sobre cual dato esta trabajando (esto lo hace en background en memoria local al thread).
    Este es el nodo actualmente activo del grafo de dependencias.
    Por otro lado, el dato que necesita ser leido para computar el valor del nodo actual tambien se registra: usualmente ya reside en algun tipo de contenedor (e.g. una tabla hash) que requiere invocar un metodo de lookup para acceder a una entrada especifica.
    Se hace uso de este hecho, haciendo que los metodos de lookup crean los arcos del grafo de dependencia, y sencillamente se agregan al grafo.
    Al final de las sesiones de compilacion se tienen todos los nodos enlazados, automaticamente.

    FIGURE Dependency Graph of Compilation Data

    Este grafo de dependencia se almacena en un directorio de cache de compilacion incremental, junto con las entradas que el cache describe.
    Al comienzo de una sesion de compilacion siguiente, se detecta cuales entradas (i.e. nodos del AST) han cambiado, comparandolas con sus versiones previas.
    Dado el grafo y el conjunto de entradas que cambiaron, se puede facilmente encontrar todas las entradas en el cache que no estan actualizadas y eliminarlas del cache.
    Cualquier elemento que sobrevivio esta fase de validacion de cache puede ser re-utilizado durante la sesion de compilacion actual.

    FIGURE Using the Dependency Graph to Validate the Incremental Compilation Cache

    There are a few benefits to the automated dependency tracking approach we are employing. Since it is built into the compiler's internal APIs, it will stay up-to-date with changes to the compiler, and it is hard to accidentally forget about. And if one still forgets using it correctly (e.g. by not declaring the correct active node in some place) then the result is an overly conservative, but still "correct" dependency graph: It will negatively impact the re-use ratio but it will not lead to incorrectly re-using some outdated piece of data.

    Tambien es de notar que el sistema no intenta predecir o computar como sera el grafo de dependencia.
    Una gran parte de las pruebas de regresion, sin embargo, si tendran una descripcion de como deberia ser el grafo de dependencia para un programa dado.
    Esto asegura que el grafo efectivo y el grafo de referencia se construyen por distintos metodos, reduciendo el riesgo de que tanto el compilador y el test esten de acuerdo en un valor incorrecto.

    Algunas implicaciones:
    El grafo de dependencia refleja las dependencias efectivas entre partes del codigo fuente y partes del binario emitido.
    Si existe un nodo de entrada que es alcanzable de muchos nodos intermedios, e.g. un tipo de dato central que es utilizado en casi toda funcion, entonces cambiar la definicion de ese tipo de dato causara que casi todo debe ser compilado de cero.

    En otras palabras, la efectividad de la compilacion incremental es muy sensible a la estructura del programa siendo recompilado y al cambio realizado.
    Cambiar un unico caracter del codigo fuente podria invalidar completamente el cache de compilacion incremental.
    Sin embargo, este tipo de cambio es un caso raro y la mayor parte del tiempo solo una pequeña porcion del programa debe ser recompilado.

The Current Status of the Implementation

    El estado actual de rustc a fines de 2019.
    Para la primera implementacion de compilacion incremental, implementada a principios de 2019, el equipo de rustc de focalizo en cachear archivos objeto.
    Consequentemente, si esta fase se puede saltear aunque sea para parte de un codigo, se puede lograr el mayor impacto en tiempos de compilacion.
    Con esto en mente, tambien se puede estimar la cota superior de cuanto tiempo se puede ahorrar: si el compilador pasa N segundos optimizando cuando compila un crate, entonces la compilacion incremental puede reducir los tiempos de compilado en a lo sumo esos N segundos.
    Otra area que tiene una gran influencia en la efectividad de la primera implementacion de es la granularidad del seguimiento de dependencias.
    Depende de la implementacion cuan fina es la granularidad de los grafos de dependencias, y la implementacion actual es media gruesa.
    Por ejemplo, el grafo de dependencias solo tiene un unico nodo para todos los metodos en un \texttt{impl} (bloque de implementacion de un trait).
    En consecuencia, el compilador considerara que cambiaron todos los metodos de ese \texttt{impl} aunque solo haya cambiado uno solo.
    Esto por supuesto significa que mas codigo sera recompilado de lo que seria estrictamente necesario.

Future Plans

    The alpha version represents a minimal end-to-end implementation of incremental compilation for the Rust compiler, so there is lots of room for improvement. The section on the current status already laid out the two major axes along which we will pursue increased efficiency:

        Cache more intermediate results, like MIR and type information, which will allow the compiler to skip more and more steps.

        Make dependency tracking more precise, so that the compiler encounters fewer false positives during cache invalidation.

    Improvements in both of these directions will make incremental compilation more effective as the implementation matures.

    In terms of correctness, we tried to err on the side of caution from the get-go, rather making the compiler recompute something if we were not sure if our dependency tracking did the right thing, but there is still more that can be done.

        We want to have many more auto-tests that make sure that various basic components of the system don't regress. This is an area where interested people can start contributing with relative ease, since one only needs to understand the Rust language and the test framework, but not the more complicated innards of the compiler's implementation. If you are interested in jumping in, head on over to the tracking issue on GitHub and leave a comment!

        We are working on the cargo incremental tool (implemented as a Cargo subcommand for hassle-free installation and usage) that will walk a projects git history, compiling successive versions of the source code and collecting data on the efficiency and correctness of incremental versus regular compilation. If you're interested in helping out, consider yourself invited to either hack on the tool itself or downloading and running it on a project of yours. The #rustc channel on IRC is currently the best place to get further information regarding this.
