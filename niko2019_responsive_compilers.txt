\addcontentsline{toc}{section}{Responsive Compilers}
\section*{Responsive compilers - Nicholas Matsakis - PLISS 2019}
\cite{niko2019responsive}

# pipelines and passes

  when I started writing compilers, we used the dragon book I learned this classic structure of how to write a compiler which was all about passes

  that is precisely how rustc used to look

  the reason that there has been a change is that the way you interact with comilers has changed

  the way the rust compiler was written and the way compilers usually work is this batch compilation model

  where you run it, process the whole source and you produce an output and maybe get an error out of it

  these days people are working with ides and you want a different way to interact with the source when your in this model

  you want to take messed up inputs, make sense of them, you want to be able to do compiletions and jump to definitions in an interactive way

  my goal today: what if the dragon book were written today

  i think there are more questions marks than answer, we dont have all the things written down to make the book

  but I have a lot of experiences of what we tried and the challenves

  the first thing to learn about this environment today is that there has been a big shift the last couple of years in how ides are written is that microsoft introduced vs code, which is an amazing editor,
  but among the many amazing things it introduced is the LSP, which is an intermediate protocol for interfacing between the language that's being compiled and the editor that's interacting, so that neither have to be tied to each other

  it used to be that when you wrote an eclipse plugin for your language it just worked in exclsipse, and then if you wanted to extend for netbeans, and emacs, and vim, etc
  but LSP lets you sidestep that

  so for example in the rust compiler we have a language service, we actually ahve to and they work for emacws, vi, whatever

# The "responsive" compiler

  - compiler as an actor
  - editor sends diffs and requests completions, diagnostics
  - compiler responds

  what you wind up in this model is that instead of having the compiler be something that you run in the command line it's more of an actor, the editor is sending you diffs of the files youre compoiling, and you are responding to them and sending back diagnostics,
  and it might say, ok, we want to know what are the completions at this point, and your compiler responds with a vector of responses

  key point: need to be able to respond as quickly as possible

  so you wind up with a pretty different structure

  andd you ahve to think about what is the minimum information you need to answer that requests, and can you process just that so you can get responses as fast as possible

  so instead of a type checker that walks all the sources and does all the things for all the functions, ok i just need to type  check this statement, how much context do i need to do that

  and i go back out to the function, i have to go back out to other functions sometimes, but not all of them, find their signatures, etc

# demand driven

  so what weve been trying to do is move to a more demand driven architecture

  - start from goal
  - figure out what is needed for that

  you have a given goal you need to do, and that goal is implemented by some function that uses other functions, and you go backwards, but you try to keep this set to a minimum
  at the end of the day you still have thesed traditional compiler passes, logical, but you might not be executing them completely and you might now tbe executing them in order

# why should you care about IDEs?

  there are some things ive noticed in trying to make this transformation that surprised me
  and there some reasons to try to do an ide friendly approach, even if its not a full ide, from the beginning

  - you have to write code in this language your making
  - it really informs your language design,
    you becoime much more aware of what depoendencies you need
    to figure out bits of information and that might lead you
    to make or not make certain desicions
  - stric phase separation is impossible anyway

# dependencies matter

  rust allows arbitrryu nesting of delcarations inside fuinctions
  an example of something we would not have done had we implemented ides earlier on
  rust has always allowed you to nest things rather arbitrarily for example a function with a struct inside of it
  thats kinda handy, sometimes you want some local data that's not needed outside the function

  fn foo() {
    // Equivalent to a struct declared at the root of the
    // file, but only visible inside this function.
    struct Bar{}
    let x = Bar{};
  }

  you can also put methods on the struct

  fn foo() {
    struct Bar{}
    impl Bar {
      pub fn method() { ... }
    }
    let x = Bar{};
  }

  a side effect of this is that auto-completion requires looking inside a lof of function bodies
  what tha means is that I could have a struct visible from outside the function, and put methods on it inside the function, and I can call those methods from outside the function,
  because the methods are dispatched based on the type, and we attached the method to type Bar, and if I have an instance of Bar from outsidfe the function I can call and what that means is that I'm doing completion on a value of type Bar, I really need to parse the inside of my function bodies to figure out if there is an impl that muight be releant, or else I wont get those methods

  struct Bar {}
  fn some_method() {
    let bar = Bar::new();
    bar. // <-- what methods should we offer as
         // auto-completion here?
  }

# strict phase separation is impossible anyway

  the other reason that led us in this direction, is that in most compiler that I've worked on , if you have a strict phase spearation in which you fully resolve all the symbols, then type check all the bodies, and then... it e3nds up kind of constraining, and you often need to process your source in a difficult order
  in many languages its a constraint you dont want

  Rust, for example, lets you do this:

  const LEN: u8 = 1 + 1 + 1;
  const DATA: [u8; LEN] = [1, 1, 1]:

  what this means now is that there is an interdependency, in order to know the full type of DATA, I have to evaluate the constant LEN, and in order to evaluate the constant LEN, I have to type check its body and execute in some way, interpret it, symbolically execute it, to figure out what it's value is, and that means I can't fully type check all the constants in one order, without considering the dependencies between them before I can even figure out the type of data.

  What we used to do is some horrible hack.

  We essentially had two implementations of some parts of the compiler, because we needed to have some subset of the typechecker and evaluator that was good enough to evaluate things like LEN and that could execute at any part on demand, and then we had the real code that did the full check that came after.

  It was a horrible pain.

  And now in this more demand based system this isnt a problem because we can go and execute LEN on its own.

  What if we were to do this?

  const LEN: u8 = DATA[0] + DATA[1] + DATA[2];
  const DATA: [u8; LEN] = [1,1,1];

  Usually when you're doing this sort of thing you end up needed to detect cycles and this kind of falls out out of the framework we're working on basically.

  Other examples of phase separation:
  - inferred types across function boundaries in e.g ML
  - in rust, theres a bunch of things in the logic language: specialization, which requires solving some traits
  - java and its lazy class file loading, how many different things the dot operator does in java, you will find that there is a bunch of lazyness, the set of classes a given classs can touch is determined as you walk the file you compile
  - how racket deals with phase separation and scheme macros

  in a lot of languages you find you want to evaluate some subset of the source and type check and be able to woirk with thme without necesarily processin ght whoel thing

# Not a solved problem

  what have we been actually doing to solve this

  - hand coded
  - salsa
  - more formal technique
    - attribute grammars
    - datalog or structured queries

  rustc takes an approach based on a framework called salsa it enables you to still write your compiler in a general purpose programming language what feels familiar

  there are two different alternative to this,

  one of them, the hand coded version, ive seen more in practice in other places, is not having a framework but thinking very carefully about things, doing the same things but open coded, doing things by hand figuring out if im going to type check this, i need to figure out these dependencies, and making it work
  that is of course very practical, it just can have bugs, incremental inconsistencies, etc if you have forgotten about a dependency between things
  another way is more formal, higher level expression
  then you have to make sure that everything you do fits in to this framework or extend the framework
  so salsa is kind of a middle ground

# Salsa

  - high level idea:
    - inputs
    - derived queries

  the high level idea of this framework is that you separate out the inputs to your compilation and then a bunch of derived stuff.

  the derived queries are basically pure functions that get to demand other results/queries, that they needs, some of which may be inputs and when we use one of these functions we track what bits of data did it use and ultimately which inputs did it use, and then whene theres a cahnge to one of these inputs we can propagate the change and try to avoid re executing some of these things.

  there are a lot of systems in this space
  the three that I know of are these
  - most closely related work:
    - adapton
    - glimmer from the ember web framework
    - build systems a la carte

  two of these are academic, adapton approach by mathew hammer and BSALC a paper by simon peyton jones that built a very
  flexible system in haskell that has a similar basis
  teh difference would be that ours is in between BSALC allows you to customize a lot of different thing and tweak many things, adapton is also a little more flexible but also more
  complicated

  an interesting approach is glimmer engine in the ember web framework, which does incremental updates. i havent looked deeply at what react or elm does but i imagine they are kinda related

  turns out this is a problem that applies to many areas, not just compilation

# Salsa core idea

  \begin{minted}{rust}
  let mut db = Database::new();
  loop {
    db.set_input_1( ... );
    db.set_input_2( ... );

    db.derived_value_1();
    db.derived_value_2();
  }
  \end{minted}

  when you are writing a program in salsa it kinda looks like this and then you essentialy have a loop where you set some inputs, and that's like when you get a diff from the editor, and you compute some derived values
  and the idea is that these things are memoized, so whenever you ask for a derived value it will always be up to date, for whatever the input changes you have made

# Entity component System

  - entity: unit of entity
  - component: data about an entity

  \begin{minted}{rust}
  fn move_left(entity: Entity, amount: usize) {
    let pos = DB.position(entity);
    pos.x -= amount;
    DB.set_position(entity, pos);
  }
  \end{minted}

  \begin{Verbatim}
  when you really try to write a whole compiler withj this kind of model one of the things that arises is this relationship to ECS, this is kind of a right turn but I want to explain this as a sort of background as a way to think about how this feels in practice

  an ECS is something that arises from game proigramming and is an alternative to OOP

  where you separate out data and the identity

  so in an OOP you have a class and you make an instance of it and all of iots dasta and its operation are defined at that moment when you created it

  whereas in an ECS you create a new entitty aND it has nothing associated with thi but identity, and then you can have separately data that you attache to it

  in games this is useful because of the very dynamic nature of data in a game.

  it allows you to be unstructured and in a compilar that part is not as importnat but its pretty useful

# Entities in a compiler

  - often called symbols
  - things like
    - input files
    - struct declarations
    - fields
    - function declarattions
    - parameters or local variables
  - something "addressable" by other parts of the system

  so what you wind iup with is a system where you entities correspond to things that get declared in the program language and you layer on different bits of data about this symbols so you might have a type, etc

  and ther reason you layer this on is this is what allows us to be so demand driven

  we can ask about the type of a symbol and get that without getting all the other bits of data that might eventually come to be associated with it

# Components in a compiler

  things like
  - the type of an entity
  - the signature of a function

  there are a couple different things that are like components, type is one, signature is another, sometimes there are more like unit results, or lists of errors, the result of applying some analysis that can reject

# Salsa queries

  Q(K0 .. Kn) -> V
  - Q is the query name (like AST)
  - the K0 .. Kn are the query keys
    - atomic values of any type
  - The V is the value associated with the query

  the basic structure of this salsa  systems is that you have queries, which have a name
  its not quite an ECS in that we dont have a formal concept of entities, instead what we have is queries which are kind of like components
  a query name might be like the type or the signature
  and then we have a set of keys that go into the query
  and often there is only one
  so the type of a given funciton, or the type of a given variable,
  but sometimes there are more than one, so there can be any number
  and when yopu execute this query you get back some value that is the rssult of these things, the keys and the values have to be values in the saense they can be copied and can be compared for equality simple values

# Example queries

  input_text(FileName)
  ast(FileName) -> Ast
  signature(Entity) -> Signature
  completions(FileName, Line, Column) -> Vec<String>

  some examples of the kinds of queries we might have
  they range from
  - base inputs, what is input text / the source in this file?
  - derived things like the ast or a signature
  - high level operations, the cursor is at this file, this line, this column,
    what is the set of strings we want to display to the user aws possible
    completions

# Query group

  #[salsa::query_group]
  trait CompilerDatabase {
    #[salsa::input]
    fn input_text(&self, filename: FileName) -> String;
    fn ast(&self, filename: FileName) -> Ast;
    fn signature(&self, entity: Entity) -> FnSignature;
  }

  you wind up structuring your program into these groups of queries
  you can think of it as a kind of interface
  you declare that the database of data is going to have these range of
  operations, some of them are inputs, these are the things you can explicitly set, and the rest are derived

  so we might say what is the ast associated with the given file, and its going to give back an ast
  so you see I can have different kinds of parameters at different points e.g. the signature of some entity in the system

  why do I separate the inputs from the deriving functions?
  it doesnt matter for the interfacem they both appear to be functions that you can invoke
  but the actual implementation of this uses a procedural macro that generates some glue code and memoization code, and for inputs it generates different code than for derived queries, and it also generates a setter

# Input queries

  //  #[salsa::input]
  //  fn input_text(&self, filename: FileName) -> String;
  let text = db.input_text(filename);
  db.set_input_text(filename, text);

  Input queries are essentially a field:
      The #[salsa::input] annotation generates accessors

  when you have the input queries, what you wind up with essentially is a kind of hashmap that storing your base data, and the framework will automatically generate a setter that you can use to set the value, and you can also just use it as a method to get the value

# Derived Queries

  // fn ast(&self, filename: FileName) -> Ast;
  fn ast(
    db: &impl CompilerDatabase,
    filename: FileName,
  ) -> Ast {
    let input_text = db.input_text(filename);
    ... /* implement parser here */ ...
  }

  Derived queries are defined by a function:
      The db parameter is the database, gives access to other queries
      Given keys, return results

  the derived queries are a little bit different
  for each one you give a function and this function takes a first argument
  which is always the database, this is some type that implementes this trait or interface
  this function only gets to work with the methods that are exposed in that interface, that's all it has access to
  these other arguments are the inputs the qquriey keeps

  one subtle but important thing is that in rust, if you have a top level function like this it has access to nothing else
  you can make global mutable data if you really want to, its difficult
  but basically it constrained in what it can access

  this is importnat because we went through a couple of iterations in our compiler

  we did three incremental systems
  one failed early
  one we got to work but it didnt work really well, thats the one which wasnt as strict as this, since we though how hard can it be to make sure you dont access things you arent supposed to access?
  it turns out its really hard and there were many subtle leaks of information where we were using data that we thought it owuld be ok to read but it was influenced by earlier sessions of compilation, and we had a lot of bugs
  that version never made it to users

  we rewrote a third timne and took this much stricted approach
  when you implement something you really dont have access to anything that is not tracked in some way and that was much better
  that is kind of like the equivalent of putting a typoe system onto your language

# How salsa works

  db.ast("foo.rs")

  Database contains:
      global revision counter
      one map per query (e.g., ast)
      maps query key to:
          Memoized result
          Vector of dependencies ([input_text("foo.rs")])
          Revision when last changed

  im going to dive a little bit into the actual implementation
  the idea is when you invoke one of these methods to compute some value i mentioned earlier that its memoized
  within a given revision, if you never change the inputs, you can think of it as a big memoization system
  so you invoke one of these methods, we're going to look up if we've computed it before, and if so well just give yo uthe value and otherwise execute your function and give you the value when its done,
  and were going to track looking for cycles while where doing that, because when we execute one function it might invoke other queries so then we can detect if there is some kind of cyclic dependenecy

  that's what we do within one revision, but then across revisions we also track what the dependencies were, so when one thing executed another we can track that and use that to figure out what might have changed

  the data we use to do this is basically a global revision counter, a map like
  a hashmap for every query, and that maps the key to the value we need,
  that's both the result, the vector of dependencies
  and its a kind of revision that tracking in what version dis this last change

  when you initially computed thats just the current revision but well see as we
  go on that that's not always equal to the current revision

# Recomputation (simplified)

  When db.ast("foo.rs") is invoked:

  If no entry yet, execute query and store result.
  Otherwise, if any input dependency is out of date:
      Re-execute ast function, recording new result + dependencies
      Update the "last changed" revision

# Recomputation

  the basic idea, the simplest idea would be something like this
  when you invoke a given query you can check if its out of scope, otherwise you
  can walk those dependencies and see check if any of them have changed, what
  did they transitively depend on, has it changed since the last revision, and
  if so re execute

  this is kind of like what make does if you think about it
  if you think about make it has this dependency graph that it figures out then if some file if the timestamp is newer it's going to invalidate eagerly everything reachable from that function and recompute it (or from that file)
  and recompute, re execute the compiler

  and this that's what what you would effectively get if you did it this way

  this is again something we tried the first time, in our first version that never made it out, and we'll see that it has some subproblems but it basically works

  so can kind of give you an example of what might will happen here

    db.signature(..)

    Current revision: R1
    db.signature(..) -- changed in R1
    db.ast(..) -- changed in R1
    db.input_text(..) -- changed in R2

  if you are computing the signature of something and you're in revision one
  let's say to compute the signature of a function
  what we would have to figure out first
  we have to go in and invoke DB.ast and that's going to give us the ast for the
  function
  and then when we have the AST
  well to get the AST that
  how do we get the AST
  it has to parse the input that the function is in
  and to parse the input we need the input
  so we would presumably invoke DB input text
  and at that point we get this is kind of the call stack right
  at that point we get to actual input
  so we can say we know what revision it's in
  it's just whatever revision it was last set
  and that gets propagated up
  so at each point so it's effectively

  QUESTION:
  the input.txt is effectively like the source in JavaScript
  you're saying like of an HTML Dom?
  yeah it's kind of like that
  you could think of it like that in this system I think
  the difference would be
  mmm difference might be this
  that the input text would be for the entire file usually
  and some of the other things you're computing might not be on the whole file
  so you might be getting the signature of some function
  but in order to get the signature of a particular function you would have to get the ast of a particular function
  in order to get the ast of a particular function you have to find the input text for the whole file that it's in
  which kind of you can parse
  well maybe you can parse parts of the file but usually you would parse the whole file and then you can extract out just the ast part of the ast that you need and propagate that back out
  and so the point being that you don't actually have the the source tokens
  at least not trivially for a particular function
  you have more of the source tokens for the whole file
  but you can usually you'll track position information so that if you needed them
  you could subset

  QUESTION

  no it returns the whole file is what I'm saying but you might later extract
  out a slice or something

  so now you know we we have this dependency information
  so if we go to a new revision when we go to recompute the signature
  we would notice that the ast is out of date
  because it's revision is too is too old right
  it has an input that is newer than then its revision and we would recompute it

    Current revision: R2
    db.signature(..) -- changed in R1
    db.ast(..) -- changed in R1 ← out of date
    db.input_text(..) -- changed in R2

# But suppose input change is not important

  but what happens a lot with this system of course is that the actual change that you made doesn't really matter
  so the simplest example is suppose I added a comment right into the function

  Before:

  // foo.rs
  fn foo() {
    do_something();
  }

  After:

  // foo.rs
  fn foo() {
    do_something(); // FIXME
  }

  now basically when we go back to our change
  the actual ast that results is going to be the same either way
  whether or not there's a comment in there
  and so it's kind of silly then to re-execute all the things that depend on that ASD

# Recomputation (less simplified)

  so the actual algorithm that we use has this one one little twist to it
  which says we execute the function but then check if the new result you got from re-executing is actually different and if it's not, you can just leave things the way they are, and otherwise you update

  When db.ast("foo.rs") is invoked:

  If no entry yet, execute query and store result.
  If any input dependency is out of date:
  Re-execute ast function, recording new result + dependencies
  If the new result is different from old result:
      Update the "last changed" revision

  this is pretty important in the end for making things really work because
  if we apply this what will happen is we'll see that the ast is the same,
  we won't update the revision in which we changed
  so the AST is still considered to have changed in revision one and then the signature is not dirty and can be reused and those things are probably not that important but hopefully there's later computations
  like like type checks and so on that actually are
  the really convenient part about this is that you can sort of do this projection where you extract out the bits you actually needed and use that to constrain changes from propagating too far

# Order matters

  some subtle points about this basic algorithm is that the first thing is that order matters a lot
  when you're checking to see if something's out of date you actually have to check it in the same order that it executed in the first place or else you might be doing things that should never have happened

  fn a(db: &impl Database) {
    if db.b() {
      db.c();
    } else {
      db.d();
    }
  }

  Input dependencies of a:
      b, c, and d
  If b were true in R1, we execute c
  In R2, if b is false, c should never be executed

# Minimizing redundant checks

  For each memoized value, track:
      "Last changed" revision
      "Last checked" revision
  Update "last checked" revision when value is updated
  Ensures that we only execute a value once per revision

# Garbage collection

  Memoized results from previous revisions may no longer be relevant
  But GC can be quite efficient:
      Execute "master query"
      Sweep any value whose "last checked" revision was not updated
  Key idea:
      The master query doubles as the mark

# General idea

  A --> C --> E --> F --> G
              ^
              |
  B --> D ----+---> H

  Re-execute the "early steps"
  But cut off as quickly as you can

# Layering

  Common pattern:
      produce a base structure
      other queries "layer" structure on top
  Example:
      parser produces AST
      name resolution resolves names to entities
      type check adds types

# Represent layers with maps

  Give each node in the AST a numeric id AstId
  Name resolution produces Map<AstId, Entity>
  Type-check produces Map<AstId, Type>

# Rust compiler of yore

  one big ast
  nodes in the assigned a pre-index (NodeId)
  this ID was used everywhere

  fn foo() { // node 0
    let x = 3; // node 1
  }
  fn bar() { // node 2
    let y = 4; // node 3
  }

# Trees are your friends

  Entity = FileName
         | Entity "." Index

  fn foo() { // node "foo.rs".0
    let x = 3; // node "foo.rs".0.0
  }
  fn bar() { // node "foo.rs".1
    let y = 4; // node "foo.rs".1.0
  }

  ---
  Entity = FileName
         | Entity "." Name [ Index ]

  fn foo() { // node "foo.rs".foo[0]
    let x = 3; // node "foo.rs".foo[0].expr[0]
  }
  fn bar() { // node "foo.rs".bar[0]
    let y = 4; // node "foo.rs".bar[0].expr[0]
  }

  ---
  Entity = FileName
         | Entity "." Name [ Index ]

  fn foo() {} // node "foo.rs".foo[0]
  fn foo() {} // node "foo.rs".foo[1]

# Interning

  struct Entity {
    value: u32
  }
  enum EntityData {
    Root(FileName),
    Child(Entity),
  }
  #[salsa::query_group]
  trait CompilerData {
    #[salsa::intern]
    fn intern_entity(&self, data: EntityData) -> Entity;
  }

# Tree-based entities also give context

  enum EntityData {
    Root(FileName),
    Child(Entity),
  }

# Signature

  Example from earlier:

      db.signature(entity)
      db.ast(file_name)
      db.input_text(file_name)

  How do we get the file_name from the entity?

# Tightening queries with projection

  db.signature(entity) -- signature of a function
  db.ast(file_name) -- ast of the entire file

  ---
  db.signature(entity)
  db.entity_ast(entity) -- extract AST of a single entity
  db.ast(file_name)

# The "outer spine"

  Question: How do we get "all the errors in the project"?

  db.all_errors()
  db.filenames() -- get a list of all filenames
  db.entities(filename) -- for each filename, get all entities
      db.parse(filename) -- parse file to AST
  db.type_check(entity) -- for each entity, do type-check
      db.ast(entity) -- get the AST for this entity
          db.parse(filename) -- parse file to AST (memoized)

  What if the user edits a comment?
  Reparse, but that's it.

  What if the user edits a fn body?
  Reparse the file.
  Extract the AST.
  Type-check the function that changed.

# Error handling

  How not to handle an error in a compiler:

  throw new TypeError();

  Another way not to handle errors:

  if (some kind of error) {
    return fake_but_otherwise_legal_value;
  }

# Recovery from day one

  Create a sentinel value that means "bad user code here"
  Invariant:
      If you see this sentinel value, errors have been reported
      So you can feel free to suppress downstream errors
  No such thing as a "fallible" compiler operation

# Example: error type

  enum Type {
    Integer,
    Character,
    Error,
  }

# Diminishing returns

  struct MethodSignature {
    argument_types: Vec<Type>,
    return_type: Type,
  }

  Hidden assumption here?

# Handling cycles

  Salsa can't handle cycles:
      currently panics, extending to permit controlled errors
  If not careful, permitting cycles is an easy way to induce order dependence

# Example: inlining (take 1)

  fn optimized_mir(func_id: FunctionId) {
    let mut mir = db.unoptimized_mir(func_id);
    for call_site in mir.call_sites() {
      let callee_mir = db.optimized_mir(call_site.callee);
      mir.inline_call(call_site, callee_mir);
    }
  }

  fn foo() {
    bar();
  }
  fn bar() {
    ....
  }

# Example: inlining (take 2)

  fn optimized_mir(func_id: FunctionId) {
    let mut mir = db.unoptimized_mir(func_id);
    for call_site in mir.call_sites() {
      if let Ok(callee_mir) = db.try_optimized_mir(call_site.callee) {
        mir.inline_call(call_site, callee_mir);
      }
    }
  }

  Attempt and recover on cycle

# Non-deterministic results

  fn foo() {
    bar();
  }
  fn bar() {
    foo();
  }

      If I start from foo, then bar is inlined into foo
      But if I start from bar, then foo is inlined into bar

# One better approach

  Construct call graph, compute SCCs as one query
  Process one SCC at a time

# Other cases involving cycles

  From Rust:
      Name resolution
      Trait resolution
  Each has their own specific requirements

# Tracking location information ("spans")

  Various techniques:
      rustc appends all the input files to one big string
      stores 32-bit indices into that string
      compact, but hostile to incremental
  One alternative:
      in the AST node, just stores its offset from previous sibling + length
      in other nodes, track the AST node id
      recompute the starting offset when needed

# What to put in your AST

  Including whitespace, comments is useful for refactoring
  Rest of compiler doesn't care

# Incremental re-parsing

  Compiler receives "diffs" to apply to previous inputs
  Useful to be able to keep old AST trees around
  If they contain absolute spans or information, they must be rebuilt

# Example: swift red and black trees

# Alternative: "zooming out" or "zooming in"

  Eliding data is good
      especially when it can be recovered
  On-demand system is a good fit for this

# Threading

  Compiler is effectively a server
  Spawn off threads to handle requests

# Salsa's threading model

  Effectively a read-write lock:

  One master thread that applies edits
  Any number of "helper threads"
      while the helper threads are active, attempts to edit will block

# Cancellation

  Threads can check periodically for pending edits
  Easiest way to recover is to panic and unwind the thread

# Conclusion

  Start with an on-demand style
  Best practices in some areas still need to be codified
      how to represent ASTs? location information?
      how to handle cycles?

